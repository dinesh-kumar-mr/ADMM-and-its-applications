# ADMM-and-its-applications
In this review, we argue that the alternating direction method of multipliers is well suited to distributed convex optimization, and in particular to large-scale problems arising in statistics, machine learning, and related areas.

The alternating direction method of multipliers (ADMM) is an algorithm that solves convex optimization problems by breaking them into smaller pieces, each of which is then easier to handle. It has recently found wide application in a number of areas. ADMM Is a simple and powerful iterative algorithm for convex optimization problems. It is much faster than conventional methods. It has emerged as a powerful technique for largescale structured optimization.

ADMM extends the method of multipliers in such a way that we get back some of the decomposability (i.e. ability to parallelize) of standard dual ascent algorithms. It also gives us a flexible framework for incorporating many types of convex constraints. ADMM can be viewed as an attempt to blend the benefits of dual decomposition and augmented Lagrangian methods for constrained optimization.
